{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f030572a-5b35-47b7-94f7-1586f792d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample text\n",
    "texts = [\n",
    "    \"Apple is looking at buying U.K. startup for $1 billion.\",\n",
    "    \"San Francisco considers banning sidewalk delivery robots.\",\n",
    "    \"London is a big city in the United Kingdom.\",\n",
    "    \"Google, headquartered in Mountain View, unveiled the new Android phone.\",\n",
    "    \"Amazon is expanding its services in Europe.\",\n",
    "    \"Facebook rebrands to Meta as it focuses on the metaverse.\",\n",
    "    \"Tesla's stock price surged after the company's latest earnings report.\",\n",
    "    \"Microsoft announced a new cloud service aimed at developers.\",\n",
    "    \"Twitter is working on new features to improve user engagement.\",\n",
    "    \"NASA's Perseverance rover successfully lands on Mars.\",\n",
    "    \"Elon Musk plans to build a tunnel under Los Angeles.\",\n",
    "    \"IBM Watson is being used in hospitals to diagnose diseases.\",\n",
    "    \"The Eiffel Tower is one of the most famous landmarks in Paris.\",\n",
    "    \"The COVID-19 pandemic has affected businesses worldwide.\",\n",
    "    \"The New York Times reported on the recent election results.\",\n",
    "    \"The Amazon rainforest is known as the lungs of the Earth.\",\n",
    "    \"SpaceX launches another rocket into orbit.\",\n",
    "    \"The Grand Canyon is a popular tourist destination in Arizona.\",\n",
    "    \"Harvard University is one of the top universities in the world.\",\n",
    "    \"The Great Wall of China is a historic monument.\",\n",
    "    \"The movie Inception was directed by Christopher Nolan.\",\n",
    "    \"Tokyo is the capital city of Japan.\",\n",
    "    \"The Sydney Opera House is an iconic building in Australia.\",\n",
    "    \"Barack Obama was the 44th president of the United States.\",\n",
    "    \"Queen Elizabeth II is the longest-reigning monarch in British history.\",\n",
    "    \"Google's search engine is used by millions of people every day.\",\n",
    "    \"The Louvre Museum in Paris is home to the Mona Lisa.\",\n",
    "    \"The Statue of Liberty is located on Liberty Island in New York Harbor.\",\n",
    "    \"Mount Everest is the highest mountain in the world.\",\n",
    "    \"The Taj Mahal is a mausoleum located in Agra, India.\",\n",
    "    \"Bill Gates is the co-founder of Microsoft.\",\n",
    "    \"The Berlin Wall fell in 1989.\",\n",
    "    \"The Amazon River is the second longest river in the world.\",\n",
    "    \"Walt Disney founded the Disney Company in 1923.\",\n",
    "    \"The Dead Sea is one of the saltiest bodies of water in the world.\",\n",
    "    \"The Leaning Tower of Pisa is a famous tourist attraction in Italy.\",\n",
    "    \"The Boston Marathon is one of the oldest marathons in the world.\",\n",
    "    \"Venice is known for its canals and gondolas.\",\n",
    "    \"The Rocky Mountains stretch from Canada to New Mexico.\",\n",
    "    \"The Golden Gate Bridge is a suspension bridge in San Francisco.\",\n",
    "    \"The Sahara Desert is the largest hot desert in the world.\",\n",
    "    \"Pablo Picasso was a famous Spanish painter and sculptor.\",\n",
    "    \"The Vatican City is the smallest country in the world.\",\n",
    "    \"The Empire State Building is a famous skyscraper in New York City.\",\n",
    "    \"The Pacific Ocean is the largest ocean on Earth.\",\n",
    "    \"Shakespeare's plays are performed all over the world.\",\n",
    "    \"The Colosseum in Rome is an ancient amphitheater.\",\n",
    "    \"The Nile River flows through Egypt.\",\n",
    "    \"The Great Barrier Reef is the world's largest coral reef system.\",\n",
    "    \"The Burj Khalifa in Dubai is the tallest building in the world.\",\n",
    "    \"The Brooklyn Bridge connects Manhattan and Brooklyn.\",\n",
    "    \"Albert Einstein developed the theory of relativity.\",\n",
    "    \"The Pyramids of Giza are located in Egypt.\",\n",
    "    \"The Boston Red Sox are a professional baseball team.\",\n",
    "    \"The Alps are a major mountain range in Europe.\",\n",
    "    \"The Sphinx is a limestone statue in Egypt.\",\n",
    "    \"The Mediterranean Sea is connected to the Atlantic Ocean.\",\n",
    "    \"The Eiffel Tower was completed in 1889.\",\n",
    "    \"The Amazon rainforest is home to diverse wildlife.\",\n",
    "    \"The Sydney Harbour Bridge is an iconic structure in Australia.\",\n",
    "    \"The Kremlin is a fortified complex in Moscow.\",\n",
    "    \"The Atlantic Ocean is the second largest ocean in the world.\",\n",
    "    \"The Mona Lisa was painted by Leonardo da Vinci.\",\n",
    "    \"The Acropolis of Athens is an ancient citadel.\",\n",
    "    \"The Mississippi River is the second longest river in the United States.\",\n",
    "    \"The Palace of Versailles is a royal residence in France.\",\n",
    "    \"The Andes are the longest continental mountain range in the world.\",\n",
    "    \"The Galapagos Islands are known for their unique wildlife.\",\n",
    "    \"The Hollywood Sign is a landmark in Los Angeles.\",\n",
    "    \"The Arctic Circle is located in the northernmost part of the Earth.\",\n",
    "    \"The CN Tower is a famous landmark in Toronto, Canada.\",\n",
    "    \"The Blue Mosque is a historic mosque in Istanbul.\",\n",
    "    \"The Dead Sea Scrolls were discovered in the West Bank.\",\n",
    "    \"The Panama Canal connects the Atlantic and Pacific Oceans.\",\n",
    "    \"The Louvre Pyramid is a glass structure in Paris.\",\n",
    "    \"The Sydney Tower is the tallest structure in Sydney.\",\n",
    "    \"The Grand Canyon Skywalk offers a view of the canyon.\",\n",
    "    \"The Great Sphinx of Giza is a limestone statue in Egypt.\",\n",
    "    \"The Niagara Falls are located on the border between Canada and the United States.\",\n",
    "    \"The Great Pyramid of Giza is one of the Seven Wonders of the Ancient World.\",\n",
    "    \"The British Museum is located in London.\",\n",
    "    \"The Smithsonian Institution is a group of museums in Washington, D.C.\",\n",
    "    \"The Tower of London is a historic castle on the River Thames.\",\n",
    "    \"The Petronas Towers are twin skyscrapers in Kuala Lumpur, Malaysia.\",\n",
    "    \"The Amazon River basin is home to a diverse range of flora and fauna.\",\n",
    "    \"The Great Wall of China stretches over 13,000 miles.\",\n",
    "    \"The United Nations Headquarters is located in New York City.\",\n",
    "    \"The Taj Mahal was built by Emperor Shah Jahan.\",\n",
    "    \"The Burj Al Arab is a luxury hotel in Dubai.\",\n",
    "    \"The Hollywood Walk of Fame honors celebrities in the entertainment industry.\",\n",
    "    \"The Statue of Liberty was a gift from France.\",\n",
    "    \"The Golden Gate Park is a large urban park in San Francisco.\",\n",
    "    \"The International Space Station orbits the Earth.\",\n",
    "    \"The Metropolitan Museum of Art is located in New York City.\",\n",
    "    \"The Boston Tea Party was a political protest in 1773.\",\n",
    "    \"The Great Ocean Road is a scenic coastal drive in Australia.\",\n",
    "    \"The Getty Center is a museum in Los Angeles.\",\n",
    "    \"The Tower Bridge is a combined bascule and suspension bridge in London.\",\n",
    "    \"The Mount Fuji is the highest mountain in Japan.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9fc3ed-717b-4b16-939a-b930dfcc387b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff29a0a-6ef2-420e-aed3-bcb79ba6966c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 99 sentences\n",
      "Validation data: 99 sentences\n",
      "Test data: 99 sentences\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the texts with spaCy to extract sentences and named entities\n",
    "def process_data(texts):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for doc in nlp.pipe(texts):\n",
    "        sentence = []\n",
    "        label = []\n",
    "        for token in doc:\n",
    "            sentence.append(token.text)\n",
    "            label.append(token.ent_iob_ + \"-\" + token.ent_type_ if token.ent_type_ else \"O\")\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = process_data(texts)\n",
    "valid_sentences, valid_labels = process_data(texts)  # Using the same data for validation for simplicity\n",
    "test_sentences, test_labels = process_data(texts)    # Using the same data for testing for simplicity\n",
    "\n",
    "print(f\"Training data: {len(train_sentences)} sentences\")\n",
    "print(f\"Validation data: {len(valid_sentences)} sentences\")\n",
    "print(f\"Test data: {len(test_sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17be41fe-9cd7-4d8a-83d1-0347acbba467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple is looking at buying U.K. startup for $1 billion.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc123d2-761f-4283-b262-c6473a70a343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'buying',\n",
       " 'U.K.',\n",
       " 'startup',\n",
       " 'for',\n",
       " '$',\n",
       " '1',\n",
       " 'billion',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2887bedd-c6bb-462a-8751-5c511b79c402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-GPE',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MONEY',\n",
       " 'I-MONEY',\n",
       " 'I-MONEY',\n",
       " 'O']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0abb523d-761d-4e07-a05a-0aece3210134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 2,\n",
       " 'The': 3,\n",
       " 'is': 4,\n",
       " 'in': 5,\n",
       " 'the': 6,\n",
       " 'a': 7,\n",
       " 'of': 8,\n",
       " 'world': 9,\n",
       " 'to': 10,\n",
       " 'on': 11,\n",
       " 'was': 12,\n",
       " 'located': 13,\n",
       " 'are': 14,\n",
       " 'Tower': 15,\n",
       " 'and': 16,\n",
       " ',': 17,\n",
       " \"'s\": 18,\n",
       " 'New': 19,\n",
       " 'Great': 20,\n",
       " 'United': 21,\n",
       " 'Amazon': 22,\n",
       " 'one': 23,\n",
       " 'famous': 24,\n",
       " 'York': 25,\n",
       " 'River': 26,\n",
       " 'London': 27,\n",
       " 'Earth': 28,\n",
       " 'by': 29,\n",
       " 'Sydney': 30,\n",
       " 'an': 31,\n",
       " 'longest': 32,\n",
       " 'mountain': 33,\n",
       " 'Bridge': 34,\n",
       " 'largest': 35,\n",
       " 'City': 36,\n",
       " 'Ocean': 37,\n",
       " 'Egypt': 38,\n",
       " 'for': 39,\n",
       " 'San': 40,\n",
       " 'Francisco': 41,\n",
       " 'new': 42,\n",
       " 'Los': 43,\n",
       " 'Angeles': 44,\n",
       " 'Paris': 45,\n",
       " 'known': 46,\n",
       " 'Wall': 47,\n",
       " 'historic': 48,\n",
       " 'Australia': 49,\n",
       " 'States': 50,\n",
       " 'Museum': 51,\n",
       " 'home': 52,\n",
       " 'Liberty': 53,\n",
       " 'second': 54,\n",
       " 'Sea': 55,\n",
       " 'Boston': 56,\n",
       " 'Canada': 57,\n",
       " 'Giza': 58,\n",
       " 'range': 59,\n",
       " 'Atlantic': 60,\n",
       " 'structure': 61,\n",
       " 'at': 62,\n",
       " 'city': 63,\n",
       " 'Google': 64,\n",
       " 'its': 65,\n",
       " 'Europe': 66,\n",
       " 'as': 67,\n",
       " 'Microsoft': 68,\n",
       " 'used': 69,\n",
       " 'Eiffel': 70,\n",
       " 'rainforest': 71,\n",
       " 'Grand': 72,\n",
       " 'Canyon': 73,\n",
       " 'tourist': 74,\n",
       " 'China': 75,\n",
       " 'Japan': 76,\n",
       " 'iconic': 77,\n",
       " 'building': 78,\n",
       " '-': 79,\n",
       " 'British': 80,\n",
       " 'Louvre': 81,\n",
       " 'Mona': 82,\n",
       " 'Lisa': 83,\n",
       " 'Statue': 84,\n",
       " 'Mount': 85,\n",
       " 'highest': 86,\n",
       " 'Taj': 87,\n",
       " 'Mahal': 88,\n",
       " 'river': 89,\n",
       " 'Disney': 90,\n",
       " 'Dead': 91,\n",
       " 'from': 92,\n",
       " 'Golden': 93,\n",
       " 'Gate': 94,\n",
       " 'suspension': 95,\n",
       " 'bridge': 96,\n",
       " 'Pacific': 97,\n",
       " 'ocean': 98,\n",
       " 'over': 99,\n",
       " 'ancient': 100,\n",
       " 'Burj': 101,\n",
       " 'Dubai': 102,\n",
       " 'tallest': 103,\n",
       " 'Brooklyn': 104,\n",
       " 'connects': 105,\n",
       " 'Sphinx': 106,\n",
       " 'limestone': 107,\n",
       " 'statue': 108,\n",
       " 'diverse': 109,\n",
       " 'wildlife': 110,\n",
       " 'France': 111,\n",
       " 'Hollywood': 112,\n",
       " 'landmark': 113,\n",
       " 'Pyramid': 114,\n",
       " 'Apple': 115,\n",
       " 'looking': 116,\n",
       " 'buying': 117,\n",
       " 'U.K.': 118,\n",
       " 'startup': 119,\n",
       " '$': 120,\n",
       " '1': 121,\n",
       " 'billion': 122,\n",
       " 'considers': 123,\n",
       " 'banning': 124,\n",
       " 'sidewalk': 125,\n",
       " 'delivery': 126,\n",
       " 'robots': 127,\n",
       " 'big': 128,\n",
       " 'Kingdom': 129,\n",
       " 'headquartered': 130,\n",
       " 'Mountain': 131,\n",
       " 'View': 132,\n",
       " 'unveiled': 133,\n",
       " 'Android': 134,\n",
       " 'phone': 135,\n",
       " 'expanding': 136,\n",
       " 'services': 137,\n",
       " 'Facebook': 138,\n",
       " 'rebrands': 139,\n",
       " 'Meta': 140,\n",
       " 'it': 141,\n",
       " 'focuses': 142,\n",
       " 'metaverse': 143,\n",
       " 'Tesla': 144,\n",
       " 'stock': 145,\n",
       " 'price': 146,\n",
       " 'surged': 147,\n",
       " 'after': 148,\n",
       " 'company': 149,\n",
       " 'latest': 150,\n",
       " 'earnings': 151,\n",
       " 'report': 152,\n",
       " 'announced': 153,\n",
       " 'cloud': 154,\n",
       " 'service': 155,\n",
       " 'aimed': 156,\n",
       " 'developers': 157,\n",
       " 'Twitter': 158,\n",
       " 'working': 159,\n",
       " 'features': 160,\n",
       " 'improve': 161,\n",
       " 'user': 162,\n",
       " 'engagement': 163,\n",
       " 'NASA': 164,\n",
       " 'Perseverance': 165,\n",
       " 'rover': 166,\n",
       " 'successfully': 167,\n",
       " 'lands': 168,\n",
       " 'Mars': 169,\n",
       " 'Elon': 170,\n",
       " 'Musk': 171,\n",
       " 'plans': 172,\n",
       " 'build': 173,\n",
       " 'tunnel': 174,\n",
       " 'under': 175,\n",
       " 'IBM': 176,\n",
       " 'Watson': 177,\n",
       " 'being': 178,\n",
       " 'hospitals': 179,\n",
       " 'diagnose': 180,\n",
       " 'diseases': 181,\n",
       " 'most': 182,\n",
       " 'landmarks': 183,\n",
       " 'COVID-19': 184,\n",
       " 'pandemic': 185,\n",
       " 'has': 186,\n",
       " 'affected': 187,\n",
       " 'businesses': 188,\n",
       " 'worldwide': 189,\n",
       " 'Times': 190,\n",
       " 'reported': 191,\n",
       " 'recent': 192,\n",
       " 'election': 193,\n",
       " 'results': 194,\n",
       " 'lungs': 195,\n",
       " 'SpaceX': 196,\n",
       " 'launches': 197,\n",
       " 'another': 198,\n",
       " 'rocket': 199,\n",
       " 'into': 200,\n",
       " 'orbit': 201,\n",
       " 'popular': 202,\n",
       " 'destination': 203,\n",
       " 'Arizona': 204,\n",
       " 'Harvard': 205,\n",
       " 'University': 206,\n",
       " 'top': 207,\n",
       " 'universities': 208,\n",
       " 'monument': 209,\n",
       " 'movie': 210,\n",
       " 'Inception': 211,\n",
       " 'directed': 212,\n",
       " 'Christopher': 213,\n",
       " 'Nolan': 214,\n",
       " 'Tokyo': 215,\n",
       " 'capital': 216,\n",
       " 'Opera': 217,\n",
       " 'House': 218,\n",
       " 'Barack': 219,\n",
       " 'Obama': 220,\n",
       " '44th': 221,\n",
       " 'president': 222,\n",
       " 'Queen': 223,\n",
       " 'Elizabeth': 224,\n",
       " 'II': 225,\n",
       " 'reigning': 226,\n",
       " 'monarch': 227,\n",
       " 'history': 228,\n",
       " 'search': 229,\n",
       " 'engine': 230,\n",
       " 'millions': 231,\n",
       " 'people': 232,\n",
       " 'every': 233,\n",
       " 'day': 234,\n",
       " 'Island': 235,\n",
       " 'Harbor': 236,\n",
       " 'Everest': 237,\n",
       " 'mausoleum': 238,\n",
       " 'Agra': 239,\n",
       " 'India': 240,\n",
       " 'Bill': 241,\n",
       " 'Gates': 242,\n",
       " 'co': 243,\n",
       " 'founder': 244,\n",
       " 'Berlin': 245,\n",
       " 'fell': 246,\n",
       " '1989': 247,\n",
       " 'Walt': 248,\n",
       " 'founded': 249,\n",
       " 'Company': 250,\n",
       " '1923': 251,\n",
       " 'saltiest': 252,\n",
       " 'bodies': 253,\n",
       " 'water': 254,\n",
       " 'Leaning': 255,\n",
       " 'Pisa': 256,\n",
       " 'attraction': 257,\n",
       " 'Italy': 258,\n",
       " 'Marathon': 259,\n",
       " 'oldest': 260,\n",
       " 'marathons': 261,\n",
       " 'Venice': 262,\n",
       " 'canals': 263,\n",
       " 'gondolas': 264,\n",
       " 'Rocky': 265,\n",
       " 'Mountains': 266,\n",
       " 'stretch': 267,\n",
       " 'Mexico': 268,\n",
       " 'Sahara': 269,\n",
       " 'Desert': 270,\n",
       " 'hot': 271,\n",
       " 'desert': 272,\n",
       " 'Pablo': 273,\n",
       " 'Picasso': 274,\n",
       " 'Spanish': 275,\n",
       " 'painter': 276,\n",
       " 'sculptor': 277,\n",
       " 'Vatican': 278,\n",
       " 'smallest': 279,\n",
       " 'country': 280,\n",
       " 'Empire': 281,\n",
       " 'State': 282,\n",
       " 'Building': 283,\n",
       " 'skyscraper': 284,\n",
       " 'Shakespeare': 285,\n",
       " 'plays': 286,\n",
       " 'performed': 287,\n",
       " 'all': 288,\n",
       " 'Colosseum': 289,\n",
       " 'Rome': 290,\n",
       " 'amphitheater': 291,\n",
       " 'Nile': 292,\n",
       " 'flows': 293,\n",
       " 'through': 294,\n",
       " 'Barrier': 295,\n",
       " 'Reef': 296,\n",
       " 'coral': 297,\n",
       " 'reef': 298,\n",
       " 'system': 299,\n",
       " 'Khalifa': 300,\n",
       " 'Manhattan': 301,\n",
       " 'Albert': 302,\n",
       " 'Einstein': 303,\n",
       " 'developed': 304,\n",
       " 'theory': 305,\n",
       " 'relativity': 306,\n",
       " 'Pyramids': 307,\n",
       " 'Red': 308,\n",
       " 'Sox': 309,\n",
       " 'professional': 310,\n",
       " 'baseball': 311,\n",
       " 'team': 312,\n",
       " 'Alps': 313,\n",
       " 'major': 314,\n",
       " 'Mediterranean': 315,\n",
       " 'connected': 316,\n",
       " 'completed': 317,\n",
       " '1889': 318,\n",
       " 'Harbour': 319,\n",
       " 'Kremlin': 320,\n",
       " 'fortified': 321,\n",
       " 'complex': 322,\n",
       " 'Moscow': 323,\n",
       " 'painted': 324,\n",
       " 'Leonardo': 325,\n",
       " 'da': 326,\n",
       " 'Vinci': 327,\n",
       " 'Acropolis': 328,\n",
       " 'Athens': 329,\n",
       " 'citadel': 330,\n",
       " 'Mississippi': 331,\n",
       " 'Palace': 332,\n",
       " 'Versailles': 333,\n",
       " 'royal': 334,\n",
       " 'residence': 335,\n",
       " 'Andes': 336,\n",
       " 'continental': 337,\n",
       " 'Galapagos': 338,\n",
       " 'Islands': 339,\n",
       " 'their': 340,\n",
       " 'unique': 341,\n",
       " 'Sign': 342,\n",
       " 'Arctic': 343,\n",
       " 'Circle': 344,\n",
       " 'northernmost': 345,\n",
       " 'part': 346,\n",
       " 'CN': 347,\n",
       " 'Toronto': 348,\n",
       " 'Blue': 349,\n",
       " 'Mosque': 350,\n",
       " 'mosque': 351,\n",
       " 'Istanbul': 352,\n",
       " 'Scrolls': 353,\n",
       " 'were': 354,\n",
       " 'discovered': 355,\n",
       " 'West': 356,\n",
       " 'Bank': 357,\n",
       " 'Panama': 358,\n",
       " 'Canal': 359,\n",
       " 'Oceans': 360,\n",
       " 'glass': 361,\n",
       " 'Skywalk': 362,\n",
       " 'offers': 363,\n",
       " 'view': 364,\n",
       " 'canyon': 365,\n",
       " 'Niagara': 366,\n",
       " 'Falls': 367,\n",
       " 'border': 368,\n",
       " 'between': 369,\n",
       " 'Seven': 370,\n",
       " 'Wonders': 371,\n",
       " 'Ancient': 372,\n",
       " 'World': 373,\n",
       " 'Smithsonian': 374,\n",
       " 'Institution': 375,\n",
       " 'group': 376,\n",
       " 'museums': 377,\n",
       " 'Washington': 378,\n",
       " 'D.C.': 379,\n",
       " 'castle': 380,\n",
       " 'Thames': 381,\n",
       " 'Petronas': 382,\n",
       " 'Towers': 383,\n",
       " 'twin': 384,\n",
       " 'skyscrapers': 385,\n",
       " 'Kuala': 386,\n",
       " 'Lumpur': 387,\n",
       " 'Malaysia': 388,\n",
       " 'basin': 389,\n",
       " 'flora': 390,\n",
       " 'fauna': 391,\n",
       " 'stretches': 392,\n",
       " '13,000': 393,\n",
       " 'miles': 394,\n",
       " 'Nations': 395,\n",
       " 'Headquarters': 396,\n",
       " 'built': 397,\n",
       " 'Emperor': 398,\n",
       " 'Shah': 399,\n",
       " 'Jahan': 400,\n",
       " 'Al': 401,\n",
       " 'Arab': 402,\n",
       " 'luxury': 403,\n",
       " 'hotel': 404,\n",
       " 'Walk': 405,\n",
       " 'Fame': 406,\n",
       " 'honors': 407,\n",
       " 'celebrities': 408,\n",
       " 'entertainment': 409,\n",
       " 'industry': 410,\n",
       " 'gift': 411,\n",
       " 'Park': 412,\n",
       " 'large': 413,\n",
       " 'urban': 414,\n",
       " 'park': 415,\n",
       " 'International': 416,\n",
       " 'Space': 417,\n",
       " 'Station': 418,\n",
       " 'orbits': 419,\n",
       " 'Metropolitan': 420,\n",
       " 'Art': 421,\n",
       " 'Tea': 422,\n",
       " 'Party': 423,\n",
       " 'political': 424,\n",
       " 'protest': 425,\n",
       " '1773': 426,\n",
       " 'Road': 427,\n",
       " 'scenic': 428,\n",
       " 'coastal': 429,\n",
       " 'drive': 430,\n",
       " 'Getty': 431,\n",
       " 'Center': 432,\n",
       " 'museum': 433,\n",
       " 'combined': 434,\n",
       " 'bascule': 435,\n",
       " 'Fuji': 436,\n",
       " '<PAD>': 0,\n",
       " '<UNK>': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02664b9b-92a5-4609-8065-cf9d0579dad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 2,\n",
       " 'I-ORG': 3,\n",
       " 'B-GPE': 4,\n",
       " 'B-ORG': 5,\n",
       " 'I-FAC': 6,\n",
       " 'I-GPE': 7,\n",
       " 'I-LOC': 8,\n",
       " 'B-LOC': 9,\n",
       " 'B-FAC': 10,\n",
       " 'B-PERSON': 11,\n",
       " 'I-WORK_OF_ART': 12,\n",
       " 'I-PERSON': 13,\n",
       " 'B-DATE': 14,\n",
       " 'B-CARDINAL': 15,\n",
       " 'B-ORDINAL': 16,\n",
       " 'B-WORK_OF_ART': 17,\n",
       " 'B-NORP': 18,\n",
       " 'I-MONEY': 19,\n",
       " 'I-QUANTITY': 20,\n",
       " 'B-MONEY': 21,\n",
       " 'I-DATE': 22,\n",
       " 'I-NORP': 23,\n",
       " 'B-QUANTITY': 24,\n",
       " '<PAD>': 0,\n",
       " '<UNK>': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9965b7-a2b6-4789-850c-ac8bf8deaa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# Build vocabulary for words\n",
    "def build_vocab(sentences):\n",
    "    word_counts = Counter(chain(*sentences))\n",
    "    vocab = {word: idx+2 for idx, (word, _) in enumerate(word_counts.most_common())}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "word_vocab = build_vocab(train_sentences)\n",
    "label_vocab = build_vocab(train_labels)\n",
    "\n",
    "# Encode sentences and labels into numerical values\n",
    "# def encode_data(sentences, labels, word_vocab, label_vocab):\n",
    "#     encoded_sentences = [[word_vocab.get(word, word_vocab['<UNK>']) for word in sentence] for sentence in sentences]\n",
    "#     encoded_labels = [[label_vocab[label] for label in label_seq] for label_seq in labels]\n",
    "#     return encoded_sentences, encoded_labels\n",
    "\n",
    "def encode_data(sentences, labels, word_vocab, label_vocab):\n",
    "    encoded_sentences = []\n",
    "    encoded_labels = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            encoded_sentence.append(word_vocab.get(word, word_vocab['<UNK>'])) # If the 'word' is not in the list of word_vocab, it returns word_vocal['<UNK>']\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "    for label_seq in labels:\n",
    "        encoded_label_seq = []\n",
    "        for label in label_seq:\n",
    "            encoded_label_seq.append(label_vocab[label])\n",
    "        encoded_labels.append(encoded_label_seq)\n",
    "\n",
    "    return encoded_sentences, encoded_labels\n",
    "\n",
    "train_encoded_sentences, train_encoded_labels = encode_data(train_sentences, train_labels, word_vocab, label_vocab)\n",
    "valid_encoded_sentences, valid_encoded_labels = encode_data(valid_sentences, valid_labels, word_vocab, label_vocab)\n",
    "test_encoded_sentences, test_encoded_labels = encode_data(test_sentences, test_labels, word_vocab, label_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf00db7-af36-4195-ad18-44b9e580be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'buying',\n",
       " 'U.K.',\n",
       " 'startup',\n",
       " 'for',\n",
       " '$',\n",
       " '1',\n",
       " 'billion',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab396bd-6099-4bc2-9623-38821846cb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115, 4, 116, 62, 117, 118, 119, 39, 120, 121, 122, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120b7ff6-83f8-4de7-b0e7-d3284cdc7d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 2, 2, 2, 4, 2, 2, 21, 19, 19, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3454934f-fcb2-4b3d-91c0-46fa256748fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-GPE',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MONEY',\n",
       " 'I-MONEY',\n",
       " 'I-MONEY',\n",
       " 'O']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af830adc-b2ae-4f4d-8068-234a5f44a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sentences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Collate function to pad sequences to the same length within a batch\n",
    "def pad_collate_fn(batch):\n",
    "    sentences, labels = zip(*batch)\n",
    "    lengths = [len(sentence) for sentence in sentences]\n",
    "    max_len = max(lengths)\n",
    "    padded_sentences = torch.zeros(len(sentences), max_len, dtype=torch.long)\n",
    "    padded_labels = torch.zeros(len(sentences), max_len, dtype=torch.long)\n",
    "    for i, (sentence, label) in enumerate(zip(sentences, labels)):\n",
    "        padded_sentences[i, :len(sentence)] = sentence\n",
    "        padded_labels[i, :len(label)] = label\n",
    "    return padded_sentences, padded_labels, lengths\n",
    "\n",
    "# Creating datasets and dataloaders\n",
    "train_dataset = NERDataset(train_encoded_sentences, train_encoded_labels)\n",
    "valid_dataset = NERDataset(valid_encoded_sentences, valid_encoded_labels)\n",
    "test_dataset = NERDataset(test_encoded_sentences, test_encoded_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=pad_collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, collate_fn=pad_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, collate_fn=pad_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1a78c1c-3620-4c8c-952d-fdfa3663f821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_get_iterator',\n",
       " '_index_sampler',\n",
       " '_is_protocol',\n",
       " '_iterator',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'check_worker_number_rationality',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'generator',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'persistent_workers',\n",
       " 'pin_memory',\n",
       " 'pin_memory_device',\n",
       " 'prefetch_factor',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "833ec4d7-78ca-44e1-9b4d-aba2ab74146c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_is_protocol',\n",
       " 'labels',\n",
       " 'sentences']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e42e77-2ee8-48d9-9dd5-07929f450ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Apple',\n",
       "  'is',\n",
       "  'looking',\n",
       "  'at',\n",
       "  'buying',\n",
       "  'U.K.',\n",
       "  'startup',\n",
       "  'for',\n",
       "  '$',\n",
       "  '1',\n",
       "  'billion',\n",
       "  '.'],\n",
       " ['San',\n",
       "  'Francisco',\n",
       "  'considers',\n",
       "  'banning',\n",
       "  'sidewalk',\n",
       "  'delivery',\n",
       "  'robots',\n",
       "  '.'],\n",
       " ['London', 'is', 'a', 'big', 'city', 'in', 'the', 'United', 'Kingdom', '.'],\n",
       " ['Google',\n",
       "  ',',\n",
       "  'headquartered',\n",
       "  'in',\n",
       "  'Mountain',\n",
       "  'View',\n",
       "  ',',\n",
       "  'unveiled',\n",
       "  'the',\n",
       "  'new',\n",
       "  'Android',\n",
       "  'phone',\n",
       "  '.'],\n",
       " ['Amazon', 'is', 'expanding', 'its', 'services', 'in', 'Europe', '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6a746ee-b2d4-4b71-8d6a-8c30f7b1468e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[115, 4, 116, 62, 117, 118, 119, 39, 120, 121, 122, 2],\n",
       " [40, 41, 123, 124, 125, 126, 127, 2],\n",
       " [27, 4, 7, 128, 63, 5, 6, 21, 129, 2],\n",
       " [64, 17, 130, 5, 131, 132, 17, 133, 6, 42, 134, 135, 2],\n",
       " [22, 4, 136, 65, 137, 5, 66, 2]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4615aafc-2c32-4311-b01e-ebf41c2e157a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 2, 2, 2, 2, 4, 2, 2, 21, 19, 19, 2],\n",
       " [4, 7, 2, 2, 2, 2, 2, 2],\n",
       " [4, 2, 2, 2, 2, 2, 4, 7, 7, 2],\n",
       " [5, 2, 2, 2, 4, 7, 2, 2, 2, 2, 5, 2, 2],\n",
       " [5, 2, 2, 2, 2, 2, 9, 2]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4f1d1-0708-446b-a8c9-eec5d03ee281",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "361d09a8-c4c9-4e2c-a4d1-c37294da4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, label_size, embedding_dim=100, hidden_dim=128, dropout=0.2):\n",
    "        super(NERLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # Embedding layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)  # Bi-directional LSTM\n",
    "        self.fc = nn.Linear(hidden_dim * 2, label_size)  # Fully connected layer for classification\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)  # Convert words to embeddings\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)  # LSTM\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.dropout(output)  # Apply dropout\n",
    "        logits = self.fc(output)  # Output layer\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7148cfaa-c13a-4ecf-95e9-72b3bbe78e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = NERLSTM(vocab_size=len(word_vocab), label_size=len(label_vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Loss function, ignoring padding\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer\n",
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "348c43b9-d6b4-4875-bcff-7bbe5f0d1109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccf8bc13-43cd-4d91-99b1-1216fa7604c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39dad827-5f80-4b52-b14a-688d38256fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|█████████████████| 50/50 [00:00<00:00, 117.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.0176006710529326\n",
      "Validation Loss: 1.2751260662078858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|█████████████████| 50/50 [00:00<00:00, 134.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.1826233124732972\n",
      "Validation Loss: 0.9920751029253005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|█████████████████| 50/50 [00:00<00:00, 143.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.957951826453209\n",
      "Validation Loss: 0.7690255552530288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|██████████████████| 50/50 [00:00<00:00, 95.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.7390988194942474\n",
      "Validation Loss: 0.568844196498394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|█████████████████| 50/50 [00:00<00:00, 130.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.548871459364891\n",
      "Validation Loss: 0.3958691582083702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|█████████████████| 50/50 [00:00<00:00, 143.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.3838290122151375\n",
      "Validation Loss: 0.270072735697031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|█████████████████| 50/50 [00:00<00:00, 134.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.2557044042646885\n",
      "Validation Loss: 0.17061267867684365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|█████████████████| 50/50 [00:00<00:00, 143.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.1662231119349599\n",
      "Validation Loss: 0.11401844955980778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|█████████████████| 50/50 [00:00<00:00, 125.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.1181089824438095\n",
      "Validation Loss: 0.0883105655387044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|████████████████| 50/50 [00:00<00:00, 149.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.08011684902012348\n",
      "Validation Loss: 0.05184238757006824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|████████████████| 50/50 [00:00<00:00, 136.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 0.05200507882982493\n",
      "Validation Loss: 0.034854429317638276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|████████████████| 50/50 [00:00<00:00, 149.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 0.03534272873774171\n",
      "Validation Loss: 0.024503083620220422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|████████████████| 50/50 [00:00<00:00, 135.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 0.028542036134749652\n",
      "Validation Loss: 0.019027558686211705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|████████████████| 50/50 [00:00<00:00, 133.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 0.02071170271374285\n",
      "Validation Loss: 0.015615682597272099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|████████████████| 50/50 [00:00<00:00, 131.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 0.017965570343658328\n",
      "Validation Loss: 0.012587791744153946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|████████████████| 50/50 [00:00<00:00, 136.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 0.014253887976519764\n",
      "Validation Loss: 0.010396686354652048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|████████████████| 50/50 [00:00<00:00, 142.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 0.012318813279271125\n",
      "Validation Loss: 0.009014483208302409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|████████████████| 50/50 [00:00<00:00, 138.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 0.010007440359331668\n",
      "Validation Loss: 0.007853978152852506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|████████████████| 50/50 [00:00<00:00, 139.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 0.009460433484055102\n",
      "Validation Loss: 0.006811296418309212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|████████████████| 50/50 [00:00<00:00, 148.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 0.008276993136387318\n",
      "Validation Loss: 0.0060460004908964035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for sentences, labels, lengths in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        sentences, labels = sentences.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(sentences, lengths)  # Forward pass\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])  # Reshape outputs\n",
    "        labels = labels.view(-1)  # Reshape labels\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update parameters\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels, lengths in valid_loader:\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            outputs = model(sentences, lengths)  # Forward pass\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])  # Reshape outputs\n",
    "            labels = labels.view(-1)  # Reshape labels\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            valid_loss += loss.item()\n",
    "    print(f\"Validation Loss: {valid_loss / len(valid_loader)}\")\n",
    "    model.train()  # Set the model back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b702228c-e0f3-4767-b651-d246792d16b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|█████████████████| 50/50 [00:00<00:00, 116.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.9826044428348542\n",
      "Validation Loss: 1.2764761281013488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|█████████████████| 50/50 [00:00<00:00, 123.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.1736739671230316\n",
      "Validation Loss: 0.9732812517881393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|█████████████████| 50/50 [00:00<00:00, 120.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.9281163823604583\n",
      "Validation Loss: 0.7640148621797561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|█████████████████| 50/50 [00:00<00:00, 110.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.7333216911554337\n",
      "Validation Loss: 0.5666387301683425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|█████████████████| 50/50 [00:00<00:00, 113.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.5542349010705948\n",
      "Validation Loss: 0.40824559301137925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|█████████████████| 50/50 [00:00<00:00, 122.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.390787113904953\n",
      "Validation Loss: 0.27847676277160643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|█████████████████| 50/50 [00:00<00:00, 119.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.27238918632268905\n",
      "Validation Loss: 0.1839062213525176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|█████████████████| 50/50 [00:00<00:00, 125.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.17808877386152744\n",
      "Validation Loss: 0.1304255484417081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|██████████████████| 50/50 [00:00<00:00, 88.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.12385095454752446\n",
      "Validation Loss: 0.08448163066059351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|████████████████| 50/50 [00:00<00:00, 127.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.08401994625106454\n",
      "Validation Loss: 0.05673098428174853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|████████████████| 50/50 [00:00<00:00, 123.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 0.0548913268558681\n",
      "Validation Loss: 0.038196378396824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|████████████████| 50/50 [00:00<00:00, 121.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 0.04167551605962217\n",
      "Validation Loss: 0.029772678446024655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|████████████████| 50/50 [00:00<00:00, 128.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 0.032192423595115545\n",
      "Validation Loss: 0.02207568191923201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|████████████████| 50/50 [00:00<00:00, 124.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 0.02496036244556308\n",
      "Validation Loss: 0.018295284383930265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|█████████████████| 50/50 [00:00<00:00, 61.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 0.02010549057740718\n",
      "Validation Loss: 0.01412940830225125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|█████████████████| 50/50 [00:00<00:00, 59.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 0.016762969885021448\n",
      "Validation Loss: 0.011245068025309592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|█████████████████| 50/50 [00:00<00:00, 65.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 0.013401369620114565\n",
      "Validation Loss: 0.009605907674413175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|█████████████████| 50/50 [00:00<00:00, 66.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 0.011936048506759106\n",
      "Validation Loss: 0.008331100791692733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|████████████████| 50/50 [00:00<00:00, 130.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 0.010601275640074164\n",
      "Validation Loss: 0.007215471360832453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|█████████████████| 50/50 [00:00<00:00, 94.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 0.008280913601629436\n",
      "Validation Loss: 0.00613612244836986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=20):\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_loss = 0\n",
    "#         for sentences, labels, lengths in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "#             sentences, labels = sentences.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()  # Clear gradients\n",
    "#             outputs = model(sentences, lengths)  # Forward pass\n",
    "#             outputs = outputs.view(-1, outputs.shape[-1])  # Reshape outputs\n",
    "#             labels = labels.view(-1)  # Reshape labels\n",
    "#             loss = criterion(outputs, labels)  # Compute loss\n",
    "#             loss.backward()  # Backward pass\n",
    "#             optimizer.step()  # Update parameters\n",
    "#             epoch_loss += loss.item()\n",
    "#         print(f\"Epoch {epoch+1} Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()  # Set the model to evaluation mode\n",
    "#         valid_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for sentences, labels, lengths in valid_loader:\n",
    "#                 sentences, labels = sentences.to(device), labels.to(device)\n",
    "#                 outputs = model(sentences, lengths)  # Forward pass\n",
    "#                 outputs = outputs.view(-1, outputs.shape[-1])  # Reshape outputs\n",
    "#                 labels = labels.view(-1)  # Reshape labels\n",
    "#                 loss = criterion(outputs, labels)  # Compute loss\n",
    "#                 valid_loss += loss.item()\n",
    "#         print(f\"Validation Loss: {valid_loss / len(valid_loader)}\")\n",
    "#         model.train()  # Set the model back to training mode\n",
    "\n",
    "# train_model(model, train_loader, valid_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dab487e-917e-4750-8b8a-21cc8af8df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0060460004908964035\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for sentences, labels, lengths in test_loader:\n",
    "        sentences, labels = sentences.to(device), labels.to(device)\n",
    "        outputs = model(sentences, lengths)  # Forward pass\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])  # Reshape outputs\n",
    "        labels = labels.view(-1)  # Reshape labels\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)  # Get predictions\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a62833f-2f9e-4cfc-9d7b-e72cc7b3c4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "251d4352-1384-474d-8c41-fc3035d30d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d64a0-53a5-4c26-97f0-27199a519af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
