{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7dfeb8-e93d-4f6d-95e3-4599d2b26f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.4f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "%precision %.4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f310fd-7ff5-420b-bb3f-15696ba2dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grader-required-cell\n",
    "\n",
    "# Define path for file with sonnets\n",
    "SONNETS_FILE = './data/sonnets.txt'\n",
    "\n",
    "# Read the data\n",
    "with open(SONNETS_FILE) as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Convert to lower case and save as a list\n",
    "#corpus = data.lower().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8242d81-90d7-4781-8227-733ec79be76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"FROM fairest creatures we desire increase,\\nThat thereby beauty's rose might never die,\\nBut as the riper should by time decease,\\nHis tender heir might bear his memory:\\nBut thou, contracted to thine own bright eyes,\\nFeed'st thy light'st flame with self-substantial fuel,\\nMaking a famine where abundance lies,\\nThyself thy foe, to thy sweet self too cruel.\\nThou that art now the world's fresh ornament\\nAnd only herald to the gaudy spring,\\nWithin thine own bud buriest thy content\\nAnd, tender churl, makes\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ce9546-fe7c-42fb-8fc4-23d7c58c8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data.lower().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20169e19-e8aa-487e-ad1f-b54bdaf554fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from fairest creatures we desire increase, that thereby beauty's rose might never die, but as the riper should by time decease, his tender heir might bear his memory: but thou, contracted to thine own bright eyes, feed'st thy light'st flame with self-substantial fuel, making a famine where abundance lies, thyself thy foe, to thy sweet self too cruel. thou that art now the world's fresh ornament and only herald to the gaudy spring, within thine own bud buriest thy content and, tender churl, makest waste in niggarding. pity the world, or else this glutton be, to eat the world's due, by the grave and thee. when forty winters shall beseige thy brow, and dig deep trenches in thy beauty's field, thy youth's proud livery, so gazed on now, will be a tatter'd weed, of small worth held: then being ask'd where all thy beauty lies, where all the treasure of thy lusty days, to say, within thine own deep-sunken eyes, were an all-eating shame and thriftless praise. how much more praise deserved thy b\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7816575a-df83-4e75-80b7-dda364c728ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to clean text by removing special characters\n",
    "def clean_text(text):\n",
    "    # Remove special characters using regex\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    # Return the cleaned text\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12effa83-168c-4e8f-9952-26f3b84798c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2008910-fb3f-4aaa-9b2b-e60768afc062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lightst flame with selfsubstantial fuel making a famine where abundance lies thyself thy foe to thy sweet self too cruel thou that art now the worlds fresh ornament and only herald to the gaudy spring within thine own bud buriest thy content and tender churl makest waste in niggard'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec3aaa8-30f0-49f2-89cb-8e87fa5e14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: a paragraph split into words\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6bd241-256b-47be-b726-cd45af9440a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'fairest',\n",
       " 'creatures',\n",
       " 'we',\n",
       " 'desire',\n",
       " 'increase',\n",
       " 'that',\n",
       " 'thereby',\n",
       " 'beautys',\n",
       " 'rose']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8ef042-a920-4282-8a24-22330cdc425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "CONTEXT_SIZE = 4  # Number of preceding words used to predict the next word\n",
    "EMBEDDING_DIM = 100  # Dimensionality of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3eb98a9-2ba4-4f99-9763-eba1b5e709aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create context-target pairs from the text\n",
    "def create_context_target_pairs(text, context_size):\n",
    "    context_target_pairs = []\n",
    "    for i in range(context_size, len(text)):\n",
    "        context = text[i - context_size:i]  # Get the preceding words\n",
    "        target = text[i]  # The next word\n",
    "        context_target_pairs.append((context, target))  # Append the pair\n",
    "    return context_target_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce2124fe-5374-462b-91cc-bda6ce2abdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create context-target pairs from the sample text\n",
    "context_target_pairs = create_context_target_pairs(text, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1887c9b8-a22a-49db-9869-7e6ac4ac7404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['from', 'fairest', 'creatures', 'we'], 'desire'),\n",
       " (['fairest', 'creatures', 'we', 'desire'], 'increase'),\n",
       " (['creatures', 'we', 'desire', 'increase'], 'that'),\n",
       " (['we', 'desire', 'increase', 'that'], 'thereby'),\n",
       " (['desire', 'increase', 'that', 'thereby'], 'beautys')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_target_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3fb7333-135c-42fd-96cc-3cf884e81830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary from the unique words in the text\n",
    "vocab = set(text)\n",
    "vocab_size = len(vocab)  # Number of unique words\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}  # Mapping from word to index\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}  # Mapping from index to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f00fd765-22f5-455c-8753-6744724f4455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thyself', 'grows', 'authority', 'waking', 'detain']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e48bf7-9003-4c46-97e4-5665b7c00aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3187"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69f6b700-f044-49d3-b890-9eea7db2d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fed3e2b-52ab-4be7-a97f-56db30ced95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d79df825-cb53-4b29-ae7c-3f97bdb05e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8f78f35-9940-4dd5-b2b5-ce6c4d696bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class for the context-target pairs\n",
    "class NextWordDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, context_target_pairs, word_to_idx, context_size):\n",
    "        self.context_target_pairs = context_target_pairs  # Store the pairs\n",
    "        self.word_to_idx = word_to_idx  # Store the word-to-index mapping\n",
    "        self.context_size = context_size  # Store the context size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context_target_pairs)  # Number of pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.context_target_pairs[idx]  # Get the context and target\n",
    "        context_indices = [self.word_to_idx[word] for word in context]  # Convert context words to indices\n",
    "        target_index = self.word_to_idx[target]  # Convert target word to index\n",
    "        return torch.tensor(context_indices), torch.tensor(target_index)  # Return as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5969400-d2ea-4cea-8b23-923a6005a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the dataset\n",
    "dataset = NextWordDataset(context_target_pairs, word_to_idx, CONTEXT_SIZE)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)  # Shuffle data and set batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1214d656-8173-47e4-aa9b-d392a6d7f7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4])\n",
      "torch.Size([64])\n",
      "tensor([[1624,  749,  146, 1982],\n",
      "        [1624,  928, 1768, 2204],\n",
      "        [2785, 1594, 2565,  138],\n",
      "        [1691, 1637, 3089,  849],\n",
      "        [1637,  376,  773, 1916],\n",
      "        [1982,  983,  194,  706],\n",
      "        [2302, 1905, 3080,  194],\n",
      "        [ 824, 2026,  896,  194],\n",
      "        [1768,  364,  254, 1935],\n",
      "        [2561, 1638,  194, 1441],\n",
      "        [2634, 2687, 1057, 2951],\n",
      "        [2562,  194, 2614,   99],\n",
      "        [1366, 2976, 1917, 1433],\n",
      "        [2951, 1768, 1263,  390],\n",
      "        [ 394,  194, 1066,  210],\n",
      "        [2561, 2785, 1433,  343],\n",
      "        [ 975, 3101,  824, 1637],\n",
      "        [2462,  194, 1129, 1262],\n",
      "        [2833, 1524,   59,  148],\n",
      "        [1624, 2785,  931, 1363],\n",
      "        [1391, 2802, 2146, 2843],\n",
      "        [2561,   56, 1433,  295],\n",
      "        [1445, 1074, 2501, 2943],\n",
      "        [2302, 2438,  254,   81],\n",
      "        [1192, 2718, 2799, 1840],\n",
      "        [2745,  566, 1982,  755],\n",
      "        [1624, 1614, 2293, 1248],\n",
      "        [1740, 2951, 1775, 1810],\n",
      "        [2302,  172, 1864, 1830],\n",
      "        [2739, 1637,  699, 2568],\n",
      "        [1968,  880, 1637,  584],\n",
      "        [1445, 1624,  557, 1865],\n",
      "        [2494, 2294, 1333,  177],\n",
      "        [2156, 2552,  688,  856],\n",
      "        [1982,  123, 2843,  452],\n",
      "        [2951,  146, 1333,  734],\n",
      "        [ 360, 1982, 2302, 2294],\n",
      "        [1262, 1133, 1982,  148],\n",
      "        [2692, 1300,  452, 2061],\n",
      "        [ 912, 2294, 1950, 2253],\n",
      "        [ 148, 1982, 2302, 1593],\n",
      "        [ 880, 1768, 1779, 1262],\n",
      "        [1638,  130, 2415, 2143],\n",
      "        [1323, 2899, 1445, 2900],\n",
      "        [2199, 1224,  620, 1183],\n",
      "        [ 919, 1433,  880,  393],\n",
      "        [1151, 1348, 1248, 2116],\n",
      "        [1768, 1193, 1637, 2159],\n",
      "        [  28, 1193,  880, 1262],\n",
      "        [1769,  755, 2843, 1451],\n",
      "        [3101, 1793,  188, 1662],\n",
      "        [ 364, 2951,  390, 1614],\n",
      "        [ 474, 1624,  452,  254],\n",
      "        [  28, 1627,  194, 2007],\n",
      "        [1982, 1768, 2539, 1779],\n",
      "        [  62, 2594,   43, 1793],\n",
      "        [ 128, 1504, 1678, 1982],\n",
      "        [ 148, 2302, 1413, 2785],\n",
      "        [2883, 1363, 1903,  953],\n",
      "        [ 366, 2539,  161, 1445],\n",
      "        [1624, 2785, 1433, 2247],\n",
      "        [1968, 1917,  994, 2126],\n",
      "        [1982, 1445, 1624, 2949],\n",
      "        [ 148, 1183, 1637, 1183]]) tensor([ 146, 1930,  749, 1878, 2391, 1830, 2416, 1768,  194,  973,  351, 2026,\n",
      "         148,  195, 1822, 1637, 2962,  178, 1982, 1600, 1262,  148,  220,  880,\n",
      "        1385,  543, 1796, 2561, 2367, 1189, 2951, 1220, 2951, 1624,  912,  396,\n",
      "        2951, 1096,  194,  912, 1637,  998,  211, 2843, 1441, 1729, 1067,  357,\n",
      "        1920, 1410,   71,  396,  247, 1768, 1333, 1982, 2785,   34,  146,  668,\n",
      "        1768,  521, 3097, 2785])\n"
     ]
    }
   ],
   "source": [
    "for i, (context, target) in enumerate(dataloader):\n",
    "    print(context.shape)\n",
    "    print(target.shape)\n",
    "    print(context, target)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd46702e-c505-42b5-b830-e8e7559e52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model class\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True)  # RNN layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Fully connected layer\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))  # Apply embedding and dropout\n",
    "        output, hidden = self.rnn(embedded)  # Pass through RNN\n",
    "        hidden = self.dropout(hidden[-1,:,:])  # Apply dropout to the last hidden state\n",
    "        return self.fc(hidden)  # Pass through the fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0800936-9d26-4859-9b5e-0cf4396a9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the RNN model\n",
    "HIDDEN_DIM = 128  # Number of hidden units in the RNN\n",
    "OUTPUT_DIM = vocab_size  # Output dimension equals the vocabulary size\n",
    "N_LAYERS = 2  # Number of RNN layers\n",
    "DROPOUT = 0.3  # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20afcac9-7467-4122-9382-4f7d43aa12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RNN model\n",
    "model = RNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0569a3db-3698-4693-aef5-bbda8d779584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (Cross Entropy Loss)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Define the optimizer (Adam optimizer)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6a3516e-e1d3-4e10-963c-9d1f9133da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.850768024904014\n",
      "Epoch 11, Loss: 5.0441063111715945\n",
      "Epoch 21, Loss: 4.243407128501112\n",
      "Epoch 31, Loss: 3.805284557551363\n",
      "Epoch 41, Loss: 3.5108140968058232\n",
      "Epoch 51, Loss: 3.3135825125840457\n",
      "Epoch 61, Loss: 3.1719155572626714\n",
      "Epoch 71, Loss: 3.055161089792739\n",
      "Epoch 81, Loss: 2.944371388776459\n",
      "Epoch 91, Loss: 2.8307629023155156\n",
      "Epoch 101, Loss: 2.7603789689767098\n",
      "Epoch 111, Loss: 2.71348135662775\n",
      "Epoch 121, Loss: 2.627792612044481\n",
      "Epoch 131, Loss: 2.5573419680560594\n",
      "Epoch 141, Loss: 2.503313988229654\n",
      "Epoch 151, Loss: 2.4488139709416967\n",
      "Epoch 161, Loss: 2.4204945907975635\n",
      "Epoch 171, Loss: 2.363468280238827\n",
      "Epoch 181, Loss: 2.320353685069258\n",
      "Epoch 191, Loss: 2.2821211210132515\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 200  # Number of epochs\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0  # Initialize total loss for the epoch\n",
    "    model.train()  # Set the model to training mode\n",
    "    for context, target in dataloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = model(context)  # Forward pass\n",
    "        loss = loss_function(output, target)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the parameters\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss/len(dataloader)}\")  # Print the average loss for the epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "078b198e-3346-4e86-813a-9ccbe4155996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word given a context\n",
    "def predict(context, model):\n",
    "    context_indices = [word_to_idx[word] for word in context]  # Convert context words to indices\n",
    "    context_tensor = torch.tensor(context_indices).unsqueeze(0)  # Add batch dimension and convert to tensor\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        output = model(context_tensor)  # Forward pass\n",
    "    predicted_idx = torch.argmax(output, dim=1).item()  # Get the index of the highest score\n",
    "    return idx_to_word[predicted_idx]  # Convert index back to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbd3da88-f99f-4bea-af7b-03d3af703f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['from', 'fairest', 'creatures', 'we', 'that']\n",
      "Predicted word: your\n"
     ]
    }
   ],
   "source": [
    "# Example test\n",
    "test = \"from fairest creatures we that\".split() # Define a context\n",
    "predicted_test = predict(test, model)  # Predict the next word\n",
    "print(f\"Context: {test}\")  # Print the context\n",
    "print(f\"Predicted word: {predicted_test}\")  # Print the predicted word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2b02918-1028-4e5b-821d-33c124613a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['from', 'fairest', 'creatures']\n",
      "Predicted word: the\n",
      "Context: ['fairest', 'creatures', 'the']\n",
      "Predicted word: sad\n",
      "Context: ['creatures', 'the', 'sad']\n",
      "Predicted word: skill\n",
      "Context: ['the', 'sad', 'skill']\n",
      "Predicted word: of\n",
      "Context: ['sad', 'skill', 'of']\n",
      "Predicted word: thy\n",
      "Context: ['skill', 'of', 'thy']\n",
      "Predicted word: truth\n",
      "Context: ['of', 'thy', 'truth']\n",
      "Predicted word: and\n",
      "Context: ['thy', 'truth', 'and']\n",
      "Predicted word: then\n",
      "Context: ['truth', 'and', 'then']\n",
      "Predicted word: of\n",
      "Context: ['and', 'then', 'of']\n",
      "Predicted word: welfare\n"
     ]
    }
   ],
   "source": [
    "next_words = 10\n",
    "#seed_text = \"from fairest creatures we that\".split()\n",
    "seed_text = \"from fairest creatures\".split()\n",
    "context_all = seed_text.copy()\n",
    "\n",
    "for _ in range(next_words):\n",
    "    print(f\"Context: {seed_text}\")  # Print the context\n",
    "    predicted_word = predict(seed_text, model)\n",
    "    print(f\"Predicted word: {predicted_word}\")  # Print the predicted word\n",
    "    \n",
    "    seed_text.append(predicted_word)\n",
    "    context_all.append(predicted_word)\n",
    "    seed_text = seed_text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "780aef39-c7f2-4df5-9d7a-59d1dbde9cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures the sad skill of thy truth and then of welfare\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(context_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996851f-9933-4e13-8776-727d9fbc7a39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
